# INVASE: INstance-wise VAriable SElection using neural networks \[Literature Survey]

#### Authors: Jinsung Yoon, James Jordon, Mihaela van der Schaar
#### Conference: ICLR 2019

**Problem Statement**: Instance-wise feature selection where there can be different number of features selected for each instance.  

**Motivation and Methodology**: Instance-wise feature selection was introduced by the L2X[ref] paper in 2018. It involves finding a subset of features that are most informative for each given example. But, in their methodology, the user had to provide the number of features to be selected as input. This could lead to the problem of over-explanation. So, to mitigate this problem the authors of the INVASE paper take a different approach wherein they try to minimize the KL divergence between the conditional distributions $$Y \vert X$$ and $$Y \vert X_S$$. And, in order to bypass the pain of backprop through sampling, the authors use an actor-critique framework from the RL literature. 

At this point it might not be clear how doing the above solves the issue that we had, but it does and we will see how soon.

**Methodology in detail**:

Let $$\mathcal{X}$$ be a $$d$$-dimensional feature space,

$$
\mathcal{X}=\mathcal{X}_{1} \times \ldots \times \mathcal{X}_{d}
$$

and $$\mathcal{Y}$$ be a discrete label space,

$$
\mathcal{Y}=\{1, \ldots, c\}
$$

Let $$ \mathbf{X}=\left(X_{1}, \ldots, X_{d}\right) \in \mathcal{X} $$ and $$\mathbf{Y} \in \mathcal{Y}$$ be random variables with joint p.m.f.(or p.d.f.) be $$p$$ and marginal p.m.f.(or p.d.f.) be $$p_X$$ and $$p_Y$$ respectively.

Let $$ \mathbf{s} \in\{0,1\}^{d} $$ be the selection vector, where if $$s_i=1$$ that means $$i^{th}$$ feature is selected, else if $$s_i=0$$ then the $$i^{th}$$ is not selected. Let $$∗$$ be any point not in any of the spaces $$\mathcal{X_1} , ..., \mathcal{X_d}$$ and define $$\mathcal{X}^{*}_{i} = \mathcal{X}_i \cup {∗}$$ and $$\mathcal{X}^{∗} = \mathcal{X}^{*}_{1} \times ... \times \mathcal{X}^{*}_{d}$$ . Given $$x \in \mathcal{X}$$($$x$$ is an instance of the random variable $$X$$) we will write $$x^{(s)}$$ to denote the suppressed feature vector defined by 

$$
x_{i}^{(\mathbf{s})}=\left\{\begin{array}{l}
x_{i} \text { if } s_{i}=1 \\
* \text { if } s_{i}=0
\end{array}\right.
$$

here, $$*$$ represents that the feature is not selected.

Now, this selection vector $$s$$ is generated by a selector function $S:\mathcal{X} \rightarrow \{0,1\}^{d}$$

\emph{Optimization problem}:
find $$S$$ such that $$ \left(Y \mid \mathbf{X}^{(S(\mathbf{x}))}=\mathbf{x}^{(S(\mathbf{x}))}\right) \stackrel{d .}{=}(Y \mid \mathbf{X}=\mathbf{x})$$(distributional similarity), and $$S(x)$$ is minimal i.e. as few features selected as possible.

